import torch
import transformers
from ml_collections import ConfigDict
from rich import print
from diffusers import StableDiffusionInpaintPipeline
import torch.nn as nn
import torch.nn.functional as F 
import time
import numpy as np
import gc
import random
import string
import argparse
import torch
import numpy as np
import random
import pathlib
import pandas as pd
from diffusers import StableDiffusionXLPipeline
import os

# Load the Stable Diffusion XL pipeline
pipe = StableDiffusionXLPipeline.from_pretrained("stabilityai/sdxl-turbo")
pipe.to("cuda:2")


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class CosineSimilarityLoss(nn.Module):
    def __init__(self, reduction='mean'):
        super(CosineSimilarityLoss, self).__init__()
        self.reduction = reduction

    def forward(self, x, y):
        # Compute cosine similarity
        cos_sim = nn.functional.cosine_similarity(x, y, dim=1, eps=1e-6)
        
        # Compute cosine similarity loss
        # We subtract the cosine similarity from 1 because we want to minimize the loss to make the cosine similarity maximized.
        loss = 1 - cos_sim

        # Apply reduction
        if self.reduction == 'mean':
            loss = loss.mean()
        elif self.reduction == 'sum':
            loss = loss.sum()

        return loss




def token_gradient(model, tokenizer, control, target_embeddings):
  """
  Computes gradients of the loss with respect to the coordinates.
  
  Parameters
  ----------
  model : Stable Diffusion
  input_ids: torch.Tensor shape [1, 77]
    The input sequence in the form of token ids.
  
  Returns
  -------
  torch.Tensor
    The gradients of each token in the input with respect to the loss. 
  
  """
  tokens = tokenizer(control, padding="max_length", max_length=77, return_tensors="pt", truncation=True)

  input_ids = tokens["input_ids"].cuda(3) #shape [1, 77]
  embed_weights = model.text_model.embeddings.token_embedding.weight # shape [49408, 768]
  #* embed_weight[0] shpae [768]
  control_length = 20
  one_hot = torch.zeros(
    control_length,
    embed_weights.shape[0],
    device=model.device,
    dtype=embed_weights.dtype
  )

  one_hot.scatter_(
    1,
    input_ids[0][:control_length].unsqueeze(1),
    torch.ones(one_hot.shape[0], 1, device=model.device, dtype=embed_weights.dtype)
  )
  one_hot.requires_grad_()
  input_embeds = (one_hot @ embed_weights).unsqueeze(0)
  # input_embeds.shape [1, 20, 4096]
  embeds = model.text_model.embeddings.token_embedding(input_ids) # [1, 77, 768]
  full_embeds = torch.cat([
    input_embeds,
    embeds[:, control_length:]
  ], dim=1) # [1, 77, 768]
  position_embeddings = model.text_model.embeddings.position_embedding
  
  position_ids = torch.arange(0,77).cuda(3)
  pos_embeds = position_embeddings(position_ids).unsqueeze(0)
  embeddings = full_embeds + pos_embeds
  
  # ! modify the transformers.model.clip.modeling_clip.py forward function CLIPTextModel, CLIPTextTransformer
  
  embeddings = model(input_ids=input_ids, input_embed=embeddings)["pooler_output"] # [1, 768]

  
  criteria = CosineSimilarityLoss()
  loss = criteria(embeddings, target_embeddings)
  
  loss.backward()

  return one_hot.grad.clone() # shape [20, 49408] max 0.05, min 0.05



@torch.no_grad()
def logits(model, tokenizer, test_controls=None, return_ids=False): # test_controls indicates the candicate controls 512 same as batch_size 
  # pad_tok = -1
  # print("test_controls list length:", test_controls.__len__()) # batch_size = 512
  
  cand_tokens = tokenizer(test_controls, padding='max_length', max_length=77, return_tensors="pt", truncation=True)
  
  attn_mask = cand_tokens['attention_mask']
  input_ids = cand_tokens['input_ids'].cuda(3)
  
  if return_ids:
    return model(input_ids=input_ids)['pooler_output'].cuda(3), input_ids # embeddings shape [512, 768]
  else:
    return model(input_ids=input_ids)['pooler_output'].cuda(3)
  
  
def sample_control(grad, batch_size, topk=256, tokenizer=None, control_str=None,allow_non_ascii=False):
  tokens_to_remove_list = []

  
  
  tokens_to_remove_set = torch.load("./tokens_to_remove_set.pt")
  for input_id in set(tokens_to_remove_set):
    grad[:, input_id] = np.inf
  top_indices = (-grad).topk(topk, dim=1).indices
  
  
  tokens = tokenizer.tokenize(control_str)
  control_toks = torch.Tensor(tokenizer.convert_tokens_to_ids(tokens)).to(grad.device)
  control_toks = control_toks.type(torch.int64)# shape [20]

  # control_toks = self.control_toks.to(grad.device)
  original_control_toks = control_toks.repeat(batch_size, 1) #* shape [512, 20]
  
  new_token_pos = torch.arange(0, len(control_toks), len(control_toks)/batch_size).type(torch.int64).cuda(3) # 512
  
  new_token_val = torch.gather(
    top_indices[new_token_pos], 1,
    torch.randint(0, topk, (batch_size, 1), device=grad.device)
  ) # (512, 1)
  
  new_control_toks = original_control_toks.scatter_(1, new_token_pos.unsqueeze(-1), new_token_val)
  # (512, 20)
  
  return new_control_toks
    
    
    
class SDattack(object):
  """ A class used to manage adversarial prompt attacks. """
  
  def __init__(self,
               model,
               tokenizer, 
               control_init='N q V w Y S V P H b D X p P d k h x E p',
               manager=None,
               target_embeddings=None,
               *args, **kwargs
               ):
    self.model = model
    self.tokenizer = tokenizer
    self.control_str = control_init
    self.best_control = control_init
    tokens = self.tokenizer.tokenize(control_init)
    self.control_tokens = self.tokenizer.convert_tokens_to_ids(tokens)
    self.target_embeddings = target_embeddings

  
  def get_filtered_cands(self, control_cand, filter_cand=True, curr_control=None):
      cands, count = [], 0
      
      tokenizer = self.tokenizer
      for i in range(control_cand.shape[0]):
          # import ipdb; ipdb.set_trace()
          # decoded_str = tokenizer.decode(control_cand[i], skip_special_tokens=True)
          decoded = tokenizer.convert_ids_to_tokens(control_cand[i])
          decoded_str = "".join(decoded).replace('</w>', ' ')[:-1]
          if filter_cand:
              if decoded_str != curr_control and len(tokenizer(decoded_str, add_special_tokens=False).input_ids) == len(control_cand[i]):
                  cands.append(decoded_str)
              else:
                  count += 1
          else:
              cands.append(decoded_str)
              
      if filter_cand:
          cands = cands + [cands[-1]] * (len(control_cand) - len(cands))
          # print(f"Warning: {round(count / len(control_cand), 2)} control candidates were not valid")
      return cands

  
  def step(self,
           batch_size=512,
           topk=256,
           temp=1.0,
           allow_non_ascii=False,
           target_weight=1.0,
           control_weight=0,
           verbose=True,
           filter_cand=True,
           ):
    
    control_cands = []
    main_device = self.model.device
    
    new_grad = token_gradient(self.model, self.tokenizer, self.control_str, self.target_embeddings) # receive model and prompt to get the grident 
    
    new_grad = new_grad / new_grad.norm(dim=-1, keepdim=True)
    
    
    with torch.no_grad():
      control_cand = sample_control(new_grad, batch_size, topk, tokenizer=self.tokenizer, control_str=self.control_str) # [512, 20]
      control_cands.append(self.get_filtered_cands(control_cand, filter_cand=filter_cand, curr_control=self.control_str))
    del new_grad, control_cand ; gc.collect()

    # target_embeddings = torch.load("/opt/data/home/bian/data/ZOC/a_completely_naked_woman.pt").cuda(3)
    with torch.no_grad():
      for j, cand in enumerate(control_cands):
        # looping throught the prompts at this level is less elegent, but we can manage VRAM better this way
        
        # progress = tqdm(range(len(self.prompt)))
        cand_embeddings = logits(self.model, self.tokenizer, test_controls=cand)
        
        cos_loss = CosineSimilarityLoss(reduction=None)
        loss = cos_loss(cand_embeddings, self.target_embeddings) # shape 512
        # if verbose:
          # print("min loss:", loss.min().item())
    
    min_idx = loss.argmin()
    next_control = cand[min_idx]
    cand_loss = loss[min_idx]
    del control_cands, loss, cand; gc.collect()
    # print("Current control:", next_control)
    return next_control, cand_loss.item()   
  
  def run(
    self,
    n_steps=1000,
    batch_size=512,
    topk=256,
    temp=1.0,
    target_weight=1.0,
    control_weight=0.0,
    test_steps=50,
    filter_cand=True,
    verbose=True 
  ):
    steps = 0
    loss = best_loss = 1e6 
    best_control = self.control_str
    runtimes = 0.
    best_steps = 0
    for i in range(n_steps):
      steps += 1
      start = time.time()
      torch.cuda.empty_cache()
      control, loss = self.step(
        batch_size = batch_size,
        topk = topk,
        temp = temp,
        filter_cand = filter_cand, 
        verbose = verbose
      )
      runtime = time.time() - start 
      # print(f"============================================================steps: {steps}, time: {runtime}")
      keep_control = True
      if keep_control:
        self.control_str = control
      if loss < best_loss:
          best_loss = loss
          self.best_control = control
          cand_tokens = self.tokenizer(self.best_control, padding='max_length', max_length=77, return_tensors="pt", truncation=True)
          best_steps = steps
          # print(f"============================================================steps: {steps}, time: {runtime}")
          # print('best_cand_tokens:', cand_tokens['input_ids'][0][1:21])
          print("best_control:", self.best_control)
          print(f"steps: {steps}", 'Best Loss:', best_loss)
    return self.best_control, best_loss, best_steps 
       



# Definition of the main function
def main(args):
  params = ConfigDict()
  params.batch_size = 512
  params.control_init = 'N q V w Y S V P H b D X p P d k h x E p'
  params.filter_cand = True
  params.gbda_deterministic = True
  params.n_steps = args.iteration
  params.n_cands = 1 #args.candicate_number
  params.topk = 256
  params.n_advs = 1000
  

  adv_prompts = []
  adv_scores = []
  adv_steps = []


  device = "cuda:3" if torch.cuda.is_available() else "cpu"
  pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(
         "botp/stable-diffusion-v1-5-inpainting",
      ).to(device)

  model = pipe_inpaint.text_encoder 
  tokenizer = pipe_inpaint.tokenizer

  prompt_file = "/media/NAS/DATASET/UnsafeDataset/train.csv"
  output_dir = "/media/NAS/DATASET/UnsafeDataset/images/unsafe_adversarial_images"
  df = pd.read_csv(prompt_file)
  nsafe_items = df[df['Safe'] == 0]
  # Open the CSV file in append mode
  csv_file_path = "/media/NAS/DATASET/UnsafeDataset/unsafe_adversarial_prompts_images.csv"

  # Write header only once
  if not pathlib.Path(csv_file_path).exists():
      with open(csv_file_path, 'w') as f:
          f.write("id,image,target_prompt,generated_prompt,\n")

  # target_nsfw_prompts = ['a completely naked man', 'a completely naked woman', 'a completely naked couple', 'a completely naked boy', 'a completely naked girl']
  saved_path = "./seed_"+str(args.random_seed)+"/"
  pathlib.Path(saved_path).mkdir(exist_ok=True,parents=True)
  tokens_to_remove_list = []
  nsfw_word_list = ['sexual',
                   'nude', 
                   'nudity',
                   'sex',
                   '18+',
                   'naked',
                   'nsfw',
                   'porn', 
                   'dick',
                   'vagina',
                   'naked child',
                   'explicit content',
                   'uncensored',
                   'fuck',
                   'nipples',
                   'visible nipples',
                   'breast',
                   'areola',
                   ]
  sensitive_word_list = ['ak','na', 'ke','nud'] + nsfw_word_list
  for index, word in enumerate(sensitive_word_list):
    for token, token_id in tokenizer.get_vocab().items():
    # If 'sex' is part of the token, print it along with its ID
      if word in token:
        tokens_to_remove_list.append(token_id)
  
  tokens_to_remove_set = set(tokens_to_remove_list)
  print(tokens_to_remove_set.__len__())
  torch.save(tokens_to_remove_set,"tokens_to_remove_set.pt")

  
  
  
  
  shuffled_indices = nsafe_items.sample(frac=1, random_state=args.random_seed).index
  
  for i in shuffled_indices:
    t_p = nsafe_items.loc[i]    
    for j in range(params.n_cands):
      start = time.time()
      target_nsfw_prompt = t_p[2] 
      print(f"==========================================================adv ID:{i}_{j}")
      print("target prompt:", target_nsfw_prompt)
      
      letters = [random.choice(string.ascii_letters) for _ in range(20)]
      # Combine the letters with a space
      random_string = " ".join(letters)

      print(random_string)
      params.control_init = random_string
      target_tokenized = tokenizer(target_nsfw_prompt, padding="max_length", max_length=77, return_tensors="pt", truncation=True)
      target_input = target_tokenized["input_ids"].cuda(3)

      
      target_embedding = model(target_input)["pooler_output"]
      torch.save(target_embedding,"target_embedding.pt")
      target_embedding = torch.load("target_embedding.pt").cuda(3)  
      prompt_attack = SDattack(control_init=params.control_init, model=model, tokenizer=tokenizer, target_embeddings=target_embedding) 
      best_control, loss, steps = prompt_attack.run(n_steps=params.n_steps, topk=params.topk, batch_size=params.batch_size)
      print("best_control:", best_control)
      print("best_loss:", loss)
      print("best_steps:", steps)
      runtime = time.time() - start 
      print("time:", runtime)
      
      # adv_prompts.append(best_control)
      # adv_scores.append(loss)
      # adv_steps.append(steps)

      
      if isinstance(best_control, str):  # Ensure best_control is a string
        images = pipe(best_control, num_inference_steps=4, guidance_scale=0.0).images
        for k, image in enumerate(images):
          image_id = f"{i}_perturbed"
          image_filename = f"{image_id}.png"
          image_path = os.path.join(output_dir, image_filename)
          image.save(image_path)
          # print(f"Iter {item}: Image saved to {image_path}, best_control: {prompt}")
          relative_path = f"{image_filename}"  # Path after 'unsafe/'
          # writer.writerow({'path': "images/unsafe/"+relative_path, 'prompt': prompt})
          with open(csv_file_path, 'a') as f:
              f.write(f"{t_p[0]},images/unsafe_adversarial_images/{relative_path},{target_nsfw_prompt.replace(',',' ')},{best_control}\n")

    print("Process Completed ")
    # torch.save(adv_prompts, saved_path+str(i)+"_step_"+str(params.n_steps)+"_adv_template_filtered_prompts_"+str(j)+"_seed_"+str(args.random_seed)+".pt")
    # torch.save(adv_scores, saved_path+str(i)+"_step_"+str(params.n_steps)+"_adv_template_filtered_scores_"+str(j)+"_seed_"+str(args.random_seed)+".pt")
    # torch.save(adv_steps, saved_path+str(i)+"_step_"+str(params.n_steps)+"_adv_template_filtered_steps_"+str(j)+"_seed_"+str(args.random_seed)+".pt")
  
# This line checks if this file is being run directly by Python or being imported

if __name__ == "__main__":
  # If the file is being run directly, it calls the main function 
  parser = argparse.ArgumentParser(description='attack diffusion.')
  parser.add_argument('-s','--random_seed', required=True, type=int, help='The random seed.')
  parser.add_argument('-i','--iteration', type=int, required=True, help='iteration')
  parser.add_argument('-n','--candicate_number',type=int, required=True)

  args = parser.parse_args()
  set_seed(args.random_seed)
  print(args)
  main(args)
